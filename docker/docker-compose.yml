version: "3"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.4.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-server:5.4.0
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: "true"
      CONFLUENT_SUPPORT_CUSTOMER_ID: "anonymous"
    command: >
      /bin/sh -c "((sleep 15 && kafka-topics --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 3 --topic pricing)&) &&
      /etc/confluent/docker/run"

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "8020:8020"
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop/hadoop.env
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://namenode:9870" ]
      interval: 30s
      timeout: 10s
      retries: 5
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop/hadoop.env
    volumes:
      - hadoop_datanode:/hadoop/dfs/data

  hive:
    image: bde2020/hive:2.3.2
    container_name: hive
    depends_on:
      - namenode
      - datanode
    env_file:
      - ./hadoop/hadoop-hive.env
    restart: on-failure
    ports:
      - "10000:10000"
      - "10002:10002"
    environment:
      SERVICE_PRECONDITION: "namenode:8020 hive-metastore:9083"
    volumes:
      - ./jars/:/opt/hive/aux-jars/

  hive-metastore:
    image: bde2020/hive:2.3.2
    env_file:
      - ./hadoop/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:8020 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql

  hue:
    image: gethue/hue:20211228-140101
    container_name: hue
    volumes:
      - ./hadoop/hue.ini:/usr/share/hue/desktop/conf/z-hue.ini
    ports:
      - "8888:8888"

  delta-load-tables:
    build: .
    container_name: delta-load-tables
    depends_on:
      - namenode
      - datanode
    volumes:
      - ../:/usr/local/app
    environment:
      - PYTHONPATH=$PYTHONPATH:/usr/local/app/
    command: >
      bash -c "pip install -r /usr/local/app/requirements.txt &&
      chmod 777 /usr/local/app/scripts/create_delta_tables.sh &&
      zip -r /usr/local/app/common /usr/local/app/jobs/common &&
      sh /usr/local/app/scripts/create_delta_tables.sh /usr/local/app/jobs/delta_table_create /usr/local/app/common.zip 'hdfs://namenode:8020'"

  producer:
    build: .
    container_name: producer
    depends_on:
      - broker
      - delta-stream-table
    volumes:
      - ../:/usr/local/app
    environment:
      - PYTHONPATH=$PYTHONPATH:/usr/local/app/
    command: >
      bash -c "python -m jobs.stream_producer.producer --topic pricing --servers broker:29092 --outputprefix hdfs://namenode:8020 --key id --partitionkeys 10"

  delta-stream-table:
    build: .
    container_name: delta-stream-table
    depends_on:
      - broker
      - delta-load-tables
    volumes:
      - ../:/usr/local/app
    environment:
      - PYTHONPATH=$PYTHONPATH:/usr/local/app/
    command: >
      bash -c "pip install -r /usr/local/app/requirements.txt &&
      chmod 777 /usr/local/app/scripts/process_pricing_stream.sh &&
      zip -r /usr/local/app/common /usr/local/app/jobs/common &&
      sh /usr/local/app/scripts/process_pricing_stream.sh /usr/local/app/jobs/delta_table_stream /usr/local/app/common.zip broker:29092 'hdfs://namenode:8020'"

volumes:
  hadoop_namenode:
  hadoop_datanode:
